{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Lane Finding Project\n",
    "\n",
    "This notebook contains the code to superimpose a visual cue for the road lane and a numerical estimation of its curvature and vehicle position.\n",
    "\n",
    "The steps of this project are the following:\n",
    "\n",
    "* Compute the camera calibration matrix and distortion coefficients given a set of chessboard images.\n",
    "* Apply a distortion correction to raw images.\n",
    "* Use color transforms, gradients, etc., to create a thresholded binary image.\n",
    "* Apply a perspective transform to rectify binary image (\"birds-eye view\").\n",
    "* Detect lane pixels and fit to find the lane boundary.\n",
    "* Determine the curvature of the lane and vehicle position with respect to center.\n",
    "* Warp the detected lane boundaries back onto the original image.\n",
    "* Output visual display of the lane boundaries and numerical estimation of lane curvature and vehicle position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "import pickle\n",
    "\n",
    "# Import everything needed to edit/save/watch video clips\n",
    "import imageio\n",
    "imageio.plugins.ffmpeg.download()\n",
    "from moviepy.editor import VideoFileClip\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# constants and helpers\n",
    "\n",
    "np.random.seed(54321)\n",
    "\n",
    "do_tests = False\n",
    "\n",
    "nx = 9\n",
    "ny = 6\n",
    "\n",
    "# transform points by eyeball.\n",
    "srcpoints = np.float32([[270,670],[592,450],[691,450],[1041,670]])\n",
    "dstpoints = np.float32([[270,670],[270,100],[1041,100],[1041,670]])\n",
    "imgshape = (720,1280)\n",
    "\n",
    "\n",
    "# window settings for sliding windows\n",
    "window_width = 50\n",
    "window_height = 80 # Break image into 9 vertical layers since image height is 720\n",
    "margin = 100 # How much to slide left and right for searching\n",
    "\n",
    "\n",
    "def get_cam_calib():\n",
    "    with open('cam_calib.p','rb') as f:\n",
    "        data = pickle.load(f)\n",
    "        assert data is not None\n",
    "        return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the camera calibration using chessboard images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def camera_calibration(directory='camera_cal', show_images=3):\n",
    "    # prepare object points, like (0,0,0), (1,0,0), (2,0,0) ....,(6,5,0)\n",
    "    objp = np.zeros((6*9,3), np.float32)\n",
    "    objp[:,:2] = np.mgrid[0:9,0:6].T.reshape(-1,2)\n",
    "\n",
    "    # Arrays to store object points and image points from all the images.\n",
    "    objpoints = [] # 3d points in real world space\n",
    "    imgpoints = [] # 2d points in image plane.\n",
    "\n",
    "    # Make a list of calibration images\n",
    "    images = glob.glob(directory + '/calibration*.jpg')\n",
    "\n",
    "    # Step through the list and search for chessboard corners\n",
    "    for fname in images:\n",
    "        print('.', end='')\n",
    "        img = cv2.imread(fname)\n",
    "        gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Find the chessboard corners\n",
    "        ret, corners = cv2.findChessboardCorners(gray, (nx,ny), None)\n",
    "\n",
    "        # If found, add object points, image points\n",
    "        if ret == True:\n",
    "            objpoints.append(objp)\n",
    "            imgpoints.append(corners)\n",
    "\n",
    "            # Draw and display the corners\n",
    "            img = cv2.drawChessboardCorners(img, (nx,ny), corners, ret)\n",
    "            if show_images > 0 and np.random.uniform() < .5:\n",
    "                show_images -= 1\n",
    "                plt.imshow(img)\n",
    "                plt.show()\n",
    "    print()\n",
    "    return objpoints, imgpoints\n",
    "\n",
    "if do_tests:\n",
    "    cam_calib = camera_calibration()\n",
    "\n",
    "    with open('cam_calib.p','wb') as f:\n",
    "        pickle.dump(cam_calib, f)\n",
    "    \n",
    "# objpoints, imgpoints = get_cam_calib()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Undistort and Transform Perspective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def corners_unwarp(img, mtx, dist):\n",
    "    # undistort\n",
    "    undist = cv2.undistort(img, mtx, dist, None, mtx)\n",
    "    # Convert undistorted image to grayscale\n",
    "    gray = cv2.cvtColor(undist, cv2.COLOR_BGR2GRAY)\n",
    "    # Search for corners in the grayscaled image\n",
    "    ret, corners = cv2.findChessboardCorners(gray, (nx,ny), None)\n",
    "\n",
    "    if ret == True:\n",
    "        # If we found corners, draw them! (just for fun)\n",
    "        cv2.drawChessboardCorners(undist, (nx,ny), corners, ret)\n",
    "        # Choose offset from image corners to plot detected corners\n",
    "        # This should be chosen to present the result at the proper aspect ratio\n",
    "        # My choice of 100 pixels is not exact, but close enough for our purpose here\n",
    "        offset = 100 # offset for dst points\n",
    "        # Grab the image shape\n",
    "        img_size = (gray.shape[1], gray.shape[0])\n",
    "\n",
    "        # For source points I'm grabbing the outer four detected corners\n",
    "        src = np.float32([corners[0], corners[nx-1], corners[-1], corners[-nx]])\n",
    "        # For destination points, I'm arbitrarily choosing some points to be\n",
    "        # a nice fit for displaying our warped result \n",
    "        # again, not exact, but close enough for our purposes\n",
    "        dst = np.float32([[offset, offset], [img_size[0]-offset, offset], \n",
    "                                     [img_size[0]-offset, img_size[1]-offset], \n",
    "                                     [offset, img_size[1]-offset]])\n",
    "        # Given src and dst points, calculate the perspective transform matrix\n",
    "        M = cv2.getPerspectiveTransform(src, dst)\n",
    "        # Warp the image using OpenCV warpPerspective()\n",
    "        warped = cv2.warpPerspective(undist, M, img_size)\n",
    "\n",
    "    # Return the resulting image and matrix\n",
    "    return warped, M\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def _test_calibration():\n",
    "    img = cv2.imread('camera_cal/calibration10.jpg')\n",
    "    objpoints, imgpoints = get_cam_calib()\n",
    "\n",
    "    # Camera calibration, given object points, image points, and the shape of the grayscale image:\n",
    "    ret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(objpoints, imgpoints, img.shape[0:2], None, None)\n",
    "\n",
    "    top_down, perspective_M = corners_unwarp(img, mtx, dist)\n",
    "    f, (ax1, ax2) = plt.subplots(1, 2, figsize=(24, 9))\n",
    "    f.tight_layout()\n",
    "    ax1.imshow(img)\n",
    "    ax1.set_title('Original Image', fontsize=30)\n",
    "    ax2.imshow(top_down)\n",
    "    ax2.set_title('Undistorted and Warped Image', fontsize=30)\n",
    "    plt.subplots_adjust(left=0., right=1, top=0.9, bottom=0.)\n",
    "\n",
    "if do_tests:\n",
    "    _test_calibration()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing for Lane detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def transform_perspective(img, srcpoints, dstpoints):\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    img_size = (gray.shape[1], gray.shape[0])\n",
    "    # print(img_size)\n",
    "    M = cv2.getPerspectiveTransform(srcpoints, dstpoints)\n",
    "    warped = cv2.warpPerspective(img, M, img_size)\n",
    "    return warped\n",
    "\n",
    "\n",
    "def _test_transform_perspective():\n",
    "    objpoints, imgpoints = get_cam_calib()\n",
    "    ret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(objpoints, imgpoints, imgshape, None, None)\n",
    "\n",
    "    files = glob.glob('test_images' + '/str*.jpg')\n",
    "    for imgfile in files:\n",
    "        imgori = cv2.imread(imgfile)\n",
    "        imgori = cv2.cvtColor(imgori, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        undist = cv2.undistort(imgori, mtx, dist, None, mtx)\n",
    "        img = transform_perspective(undist, srcpoints, dstpoints)\n",
    "\n",
    "        f, (ax1, ax2) = plt.subplots(1, 2, figsize=(24, 9))\n",
    "        f.tight_layout()\n",
    "        ax1.imshow(imgori)\n",
    "        ax1.set_title('Original Image', fontsize=30)\n",
    "        ax2.imshow(img)\n",
    "        ax2.set_title('Undistorted and Warped Image', fontsize=30)\n",
    "        plt.subplots_adjust(left=0., right=1, top=0.9, bottom=0.)\n",
    "\n",
    "if do_tests:\n",
    "    _test_transform_perspective()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thresholded image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def thresholded(img, s_thresh=(170, 255), sx_thresh=(20, 100)):\n",
    "    img = np.copy(img)\n",
    "    # Convert to HSV color space and separate the V channel\n",
    "    hsv = cv2.cvtColor(img, cv2.COLOR_RGB2HLS).astype(np.float)\n",
    "    l_channel = hsv[:,:,1]\n",
    "    s_channel = hsv[:,:,2]\n",
    "    \n",
    "    # Sobel x\n",
    "    sobelx = cv2.Sobel(l_channel, cv2.CV_64F, 1, 0) # Take the derivative in x\n",
    "    abs_sobelx = np.absolute(sobelx) # Absolute x derivative to accentuate lines away from horizontal\n",
    "    scaled_sobel = np.uint8(255*abs_sobelx/np.max(abs_sobelx))\n",
    "\n",
    "    # Threshold x gradient\n",
    "    sxbinary = np.zeros_like(scaled_sobel)\n",
    "    sxbinary[(scaled_sobel >= sx_thresh[0]) & (scaled_sobel <= sx_thresh[1])] = 1\n",
    "\n",
    "    # Threshold color channel\n",
    "    s_binary = np.zeros_like(s_channel)\n",
    "    s_binary[(s_channel >= s_thresh[0]) & (s_channel <= s_thresh[1])] = 1\n",
    "    \n",
    "    # Stack each channel\n",
    "    # Note color_binary[:, :, 0] is all 0s, effectively an all black image. It might\n",
    "    # be beneficial to replace this channel with something else.\n",
    "    color_binary = np.dstack(( np.zeros_like(sxbinary), sxbinary, s_binary))\n",
    "    return color_binary\n",
    "\n",
    "def _test_thresholded():\n",
    "    files = glob.glob('test_images' + '/str*.jpg')\n",
    "    for imgfile in files:\n",
    "        imgori = cv2.imread(imgfile)\n",
    "        imgori = cv2.cvtColor(imgori, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        undist = cv2.undistort(imgori, mtx, dist, None, mtx)\n",
    "        img = transform_perspective(undist, srcpoints, dstpoints)\n",
    "\n",
    "        img = thresholded(img)\n",
    "\n",
    "        # Plot the result\n",
    "        f, (ax1, ax2) = plt.subplots(1, 2, figsize=(24, 9))\n",
    "        f.tight_layout()\n",
    "        ax1.imshow(imgori)\n",
    "        ax1.set_title('Original Image', fontsize=40)\n",
    "        ax2.imshow(img)\n",
    "        ax2.set_title('Thresholded Result', fontsize=40)\n",
    "        plt.subplots_adjust(left=0., right=1, top=0.9, bottom=0.)\n",
    "\n",
    "if do_tests:\n",
    "    _test_thresholded()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Centroids using Sliding Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def window_mask(width, height, img_ref, center,level):\n",
    "    output = np.zeros_like(img_ref)\n",
    "    output[int(img_ref.shape[0]-(level+1)*height) : int(img_ref.shape[0]-level*height),\n",
    "           max(0, int(center-width/2)) : min(int(center+width/2), img_ref.shape[1])] = 1\n",
    "    return output\n",
    "\n",
    "\n",
    "def find_window_centroids(warped, window_width, window_height, margin):\n",
    "\n",
    "    window_centroids = [] # Store the (left,right) window centroid positions per level\n",
    "    window = np.ones(window_width) # Create our window template that we will use for convolutions\n",
    "\n",
    "    # First find the two starting positions for the left and right lane by using np.sum to get the\n",
    "    # vertical image slice\n",
    "    # and then np.convolve the vertical image slice with the window template \n",
    "\n",
    "    # Sum quarter bottom of image to get slice, could use a different ratio\n",
    "    l_sum = np.sum(warped[int(3*warped.shape[0]/4):,:int(warped.shape[1]/2)], axis=0)\n",
    "    l_center = np.argmax(np.convolve(window,l_sum))-window_width/2\n",
    "    r_sum = np.sum(warped[int(3*warped.shape[0]/4):,int(warped.shape[1]/2):], axis=0)\n",
    "    r_center = np.argmax(np.convolve(window,r_sum))-window_width/2+int(warped.shape[1]/2)\n",
    "\n",
    "    # Add what we found for the first layer\n",
    "    window_centroids.append((l_center,r_center))\n",
    "\n",
    "    # Go through each layer looking for max pixel locations\n",
    "    for level in range(1,(int)(warped.shape[0]/window_height)):\n",
    "        # convolve the window into the vertical slice of the image\n",
    "        image_layer = np.sum( warped[\n",
    "            int(warped.shape[0]-(level+1)*window_height) : int(warped.shape[0]-level*window_height),\n",
    "            :], axis=0)\n",
    "        conv_signal = np.convolve(window, image_layer)\n",
    "        # Find the best left centroid by using past left center as a reference\n",
    "        # Use window_width/2 as offset because convolution signal reference is at right side of\n",
    "        # window, not center of window\n",
    "        offset = window_width/2\n",
    "        l_min_index = int(max(l_center+offset-margin,0))\n",
    "        l_max_index = int(min(l_center+offset+margin,warped.shape[1]))\n",
    "        l_center = np.argmax(conv_signal[l_min_index:l_max_index])+l_min_index-offset\n",
    "        # Find the best right centroid by using past right center as a reference\n",
    "        r_min_index = int(max(r_center+offset-margin,0))\n",
    "        r_max_index = int(min(r_center+offset+margin,warped.shape[1]))\n",
    "        r_center = np.argmax(conv_signal[r_min_index:r_max_index])+r_min_index-offset\n",
    "        # Add what we found for that layer\n",
    "        window_centroids.append((l_center,r_center))\n",
    "\n",
    "    return window_centroids\n",
    "\n",
    "\n",
    "def centroids(warped):\n",
    "    # print(warped.shape)\n",
    "    window_centroids = find_window_centroids(warped, window_width, window_height, margin)\n",
    "\n",
    "    # If we found any window centers\n",
    "    if len(window_centroids) > 0:\n",
    "\n",
    "        # Points used to draw all the left and right windows\n",
    "        l_points = np.zeros_like(warped)\n",
    "        r_points = np.zeros_like(warped)\n",
    "\n",
    "        # Go through each level and draw the windows    \n",
    "        for level in range(0,len(window_centroids)):\n",
    "            # Window_mask is a function to draw window areas\n",
    "            l_mask = window_mask(window_width,window_height,warped,window_centroids[level][0],level)\n",
    "            r_mask = window_mask(window_width,window_height,warped,window_centroids[level][1],level)\n",
    "            # Add graphic points from window mask here to total pixels found \n",
    "            l_points[(l_points == 255) | ((l_mask == 1) ) ] = 255\n",
    "            r_points[(r_points == 255) | ((r_mask == 1) ) ] = 255\n",
    "\n",
    "        # Draw the results\n",
    "        template = np.array(r_points+l_points,np.uint8) # add both left and right window pixels together\n",
    "        zero_channel = np.zeros_like(template) # create a zero color channle \n",
    "        template = np.array(cv2.merge((zero_channel,template,zero_channel)),np.uint8) # make window     pixels green\n",
    "        warpage = np.array(cv2.merge((warped,warped,warped)),np.uint8) # making the original road       pixels 3 color channels\n",
    "        output = cv2.addWeighted(warpage, 1, template, 0.5, 0.0) # overlay the orignal road image with  window results\n",
    "\n",
    "    # If no window centers found, just display orginal road image\n",
    "    else:\n",
    "        print('no window centers')\n",
    "        output = np.array(cv2.merge((warped,warped,warped)),np.uint8)\n",
    "    return output\n",
    "\n",
    "\n",
    "def merge_colors(img):\n",
    "    bw = np.float32([img.shape[0],img.shape[1]])\n",
    "    bw = img[:,:,0] + img[:,:,1] + img[:,:,2]\n",
    "    return bw\n",
    "\n",
    "def _test_centroids():\n",
    "    objpoints, imgpoints = get_cam_calib()\n",
    "    ret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(objpoints, imgpoints, imgshape, None, None)\n",
    "\n",
    "    testfiles = [ 'straight_lines1', 'test2', 'test3']\n",
    "    files = [ 'test_images/'+f+'.jpg' for f in testfiles ]\n",
    "    for imgfile in files:\n",
    "        print(imgfile)\n",
    "\n",
    "        imgori = cv2.imread(imgfile)\n",
    "        imgori = cv2.cvtColor(imgori, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        undist = cv2.undistort(imgori, mtx, dist, None, mtx)\n",
    "        img = transform_perspective(undist, srcpoints, dstpoints)\n",
    "\n",
    "        img = thresholded(img)\n",
    "        \n",
    "        bw = merge_colors(img)\n",
    "        img = centroids(bw)\n",
    "\n",
    "        # Plot the result\n",
    "        f, (ax1, ax2) = plt.subplots(1, 2, figsize=(24, 9))\n",
    "        f.tight_layout()\n",
    "        ax1.imshow(imgori)\n",
    "        ax1.set_title('Original Image', fontsize=40)\n",
    "        ax2.imshow(img)\n",
    "        ax2.set_title('Centroids Result', fontsize=40)\n",
    "        plt.subplots_adjust(left=0., right=1, top=0.9, bottom=0.)\n",
    "\n",
    "if do_tests:\n",
    "    _test_centroids()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Curvature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fit_poly(centroids):\n",
    "    # print(centroids)\n",
    "\n",
    "    leftx = []\n",
    "    rightx = []\n",
    "    ys = []\n",
    "    # Go through each level and draw the windows    \n",
    "    for level in range(len(centroids)):\n",
    "        leftx.append(centroids[level][0])\n",
    "        rightx.append(centroids[level][1])\n",
    "        y = imgshape[0] - (level * window_height + window_height / 2)\n",
    "        ys.append(imgshape[0] - (level * window_height + window_height / 2))\n",
    "\n",
    "    leftx = np.array(leftx)\n",
    "    rightx = np.array(rightx)\n",
    "    ys = np.array(ys)\n",
    "    \n",
    "    # Fit a second order polynomial to pixel positions\n",
    "    left_fit = np.polyfit(ys, leftx, 2)\n",
    "    right_fit = np.polyfit(ys, rightx, 2)\n",
    "\n",
    "    return left_fit, right_fit, leftx, rightx, ys\n",
    "\n",
    "def curvature(left_fit, right_fit, leftx, rightx, ys):\n",
    "    y_eval = np.max(ys)\n",
    "    left_curverad = ((1 + (2*left_fit[0]*y_eval + left_fit[1])**2)**1.5) / np.absolute(2*left_fit[0])\n",
    "    right_curverad = ((1 + (2*right_fit[0]*y_eval + right_fit[1])**2)**1.5) / np.absolute(2*right_fit[0])\n",
    "    # print(left_curverad, right_curverad)\n",
    "\n",
    "    # Define conversions in x and y from pixels space to meters\n",
    "    ym_per_pix = 30/720 # meters per pixel in y dimension\n",
    "    xm_per_pix = 3.7/700 # meters per pixel in x dimension\n",
    "\n",
    "    # Fit new polynomials to x,y in world space\n",
    "    left_fit_cr = np.polyfit(ys*ym_per_pix, leftx*xm_per_pix, 2)\n",
    "    right_fit_cr = np.polyfit(ys*ym_per_pix, rightx*xm_per_pix, 2)\n",
    "    # Calculate the new radii of curvature\n",
    "    left_curverad = ((1 + (2*left_fit_cr[0]*y_eval*ym_per_pix + left_fit_cr[1])**2)**1.5) / np.absolute(2*left_fit_cr[0])\n",
    "    right_curverad = ((1 + (2*right_fit_cr[0]*y_eval*ym_per_pix + right_fit_cr[1])**2)**1.5) / np.absolute(2*right_fit_cr[0])\n",
    "    # Now our radius of curvature is in meters\n",
    "    # print(left_curverad, 'm', right_curverad, 'm')\n",
    "    # Example values: 632.1 m    626.2 m\n",
    "    \n",
    "    return left_curverad, right_curverad\n",
    "    \n",
    "\n",
    "def _test_curvature():\n",
    "    objpoints, imgpoints = get_cam_calib()\n",
    "    ret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(objpoints, imgpoints, imgshape, None, None)\n",
    "\n",
    "    testfiles = [ 'straight_lines1', 'test2', 'test3']\n",
    "    files = [ 'test_images/'+f+'.jpg' for f in testfiles ]\n",
    "    for imgfile in files:\n",
    "        print(imgfile)\n",
    "\n",
    "        imgori = cv2.imread(imgfile)\n",
    "        imgori = cv2.cvtColor(imgori, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        undist = cv2.undistort(imgori, mtx, dist, None, mtx)\n",
    "        img = transform_perspective(undist, srcpoints, dstpoints)\n",
    "\n",
    "        img = thresholded(img)\n",
    "\n",
    "        bw = merge_colors(img)\n",
    "        window_centroids = find_window_centroids(bw, window_width, window_height, margin)\n",
    "        l_fit, r_fit, lxs, rxs, ys = fit_poly(window_centroids)\n",
    "        l_curve, r_curve = curvature(l_fit, r_fit, lxs, rxs, ys)\n",
    "\n",
    "        # Plot the result\n",
    "        msg = \"%.0fm %.0fm\" % (l_curve, r_curve)\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        cv2.putText(img, msg,(400,100), font, 2,(120,120,120),2)\n",
    "        #Display the image\n",
    "        plt.imshow(img)\n",
    "        plt.show()\n",
    "\n",
    "if do_tests:\n",
    "    _test_curvature()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drawing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def draw(undist, warped, left_fitx, right_fitx, ploty, l_curve, r_curve):\n",
    "    \n",
    "    # Create an image to draw the lines on\n",
    "    warp_zero = np.zeros_like(warped).astype(np.uint8)\n",
    "    color_warp = np.dstack((warp_zero, warp_zero, warp_zero))\n",
    "\n",
    "    # Recast the x and y points into usable format for cv2.fillPoly()\n",
    "    pts_left = np.array([np.transpose(np.vstack([left_fitx, ploty]))])\n",
    "    pts_right = np.array([np.flipud(np.transpose(np.vstack([right_fitx, ploty])))])\n",
    "    pts = np.hstack((pts_left, pts_right))\n",
    "\n",
    "    # Draw the lane onto the warped blank image\n",
    "    cv2.fillPoly(color_warp, np.int_([pts]), (0,255, 0))\n",
    "\n",
    "    Minv = cv2.getPerspectiveTransform(dstpoints, srcpoints)\n",
    "    # Warp the blank back to original image space using inverse perspective matrix (Minv)\n",
    "    newwarp = cv2.warpPerspective(color_warp, Minv, (imgshape[1],imgshape[0]))\n",
    "    \n",
    "    # Combine the result with the original image\n",
    "    result = cv2.addWeighted(undist, 1, newwarp, 0.3, 0)\n",
    "\n",
    "    # add text\n",
    "    msg = \"%.0fm\" % ((l_curve + r_curve) / 2)\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    cv2.putText(result, msg,(500,100), font, 2,(0,0,0),2)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def _test_draw():\n",
    "    objpoints, imgpoints = get_cam_calib()\n",
    "    ret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(objpoints, imgpoints, imgshape, None, None)\n",
    "\n",
    "    testfiles = [ 'straight_lines1', 'test2', 'test3']\n",
    "    files = [ 'test_images/'+f+'.jpg' for f in testfiles ]\n",
    "    for imgfile in files:\n",
    "        print(imgfile)\n",
    "\n",
    "        imgori = cv2.imread(imgfile)\n",
    "        imgori = cv2.cvtColor(imgori, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        undist = cv2.undistort(imgori, mtx, dist, None, mtx)\n",
    "        img = transform_perspective(undist, srcpoints, dstpoints)\n",
    "        img = thresholded(img)\n",
    " \n",
    "        bw = merge_colors(img)\n",
    "        window_centroids = find_window_centroids(bw, window_width, window_height, margin)\n",
    "        l_fit, r_fit, lxs, rxs, ys = fit_poly(window_centroids)\n",
    "        l_curve, r_curve = curvature(l_fit, r_fit, lxs, rxs, ys)\n",
    "        \n",
    "        result = draw(undist, bw, lxs, rxs, ys, l_curve, r_curve)\n",
    "        \n",
    "        plt.imshow(result)\n",
    "        plt.show()\n",
    "\n",
    "if do_tests:\n",
    "    _test_draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def calibrate_camera():\n",
    "    objpoints, imgpoints = get_cam_calib()\n",
    "    ret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(objpoints, imgpoints, imgshape, None, None)\n",
    "    return mtx, dist\n",
    "\n",
    "def pipeline(imgori, mtx, dist):\n",
    "    imgori = cv2.cvtColor(imgori, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    undist = cv2.undistort(imgori, mtx, dist, None, mtx)\n",
    "    img = transform_perspective(undist, srcpoints, dstpoints)\n",
    "    img = thresholded(img)\n",
    "\n",
    "    bw = merge_colors(img)\n",
    "    window_centroids = find_window_centroids(bw, window_width, window_height, margin)\n",
    "    l_fit, r_fit, lxs, rxs, ys = fit_poly(window_centroids)\n",
    "    l_curve, r_curve = curvature(l_fit, r_fit, lxs, rxs, ys)\n",
    "\n",
    "    result = draw(undist, bw, lxs, rxs, ys, l_curve, r_curve)\n",
    "    return result\n",
    "\n",
    "params = {}\n",
    "\n",
    "def process_image(img):\n",
    "    return pipeline(img, **params)\n",
    "\n",
    "def _test_pipeline():\n",
    "    mtx, dist = calibrate_camera()\n",
    "    params['mtx'] = mtx\n",
    "    params['dist'] = dist\n",
    "    \n",
    "    files = glob.glob('test_images' + '/test[1-3].jpg')\n",
    "    for imgfile in files:\n",
    "        print(imgfile)\n",
    "\n",
    "        imgori = cv2.imread(imgfile)\n",
    "        result = process_image(imgori)\n",
    "        \n",
    "        plt.imshow(result)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "if do_tests:\n",
    "    _test_pipeline()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Process video.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] >>>> Building video advanced_lane.mp4\n",
      "[MoviePy] Writing video advanced_lane.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 1260/1261 [07:49<00:00,  2.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] Done.\n",
      "[MoviePy] >>>> Video ready: advanced_lane.mp4 \n",
      "\n",
      "CPU times: user 6min 31s, sys: 56.5 s, total: 7min 28s\n",
      "Wall time: 7min 53s\n"
     ]
    }
   ],
   "source": [
    "output_clip = 'advanced_lane.mp4'\n",
    "clip1 = VideoFileClip(\"project_video.mp4\")\n",
    "advanced_lane = clip1.fl_image(process_image)\n",
    "%time advanced_lane.write_videofile(output_clip, audio=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
